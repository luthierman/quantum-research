{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cartpole_quantum.ipynb",
      "provenance": [],
      "mount_file_id": "1mmrR4jGPGtTEdTfNVmxC7Uz4zRvrp2-e",
      "authorship_tag": "ABX9TyOimzIRNFV8HYo0bX+GxLOw",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/luthierman/quantum-research/blob/main/cartpole_quantum.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6a6SrKUI5pZR",
        "outputId": "7e862cdc-c983-4a3a-fdfa-b7ff81eb479a"
      },
      "source": [
        "!pip install tensorflow==2.3.1\n",
        "!pip install tensorflow-quantum\n",
        "!pip install gym"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==2.3.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/eb/18/374af421dfbe74379a458e58ab40cf46b35c3206ce8e183e28c1c627494d/tensorflow-2.3.1-cp37-cp37m-manylinux2010_x86_64.whl (320.4MB)\n",
            "\u001b[K     |████████████████████████████████| 320.4MB 50kB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.1) (1.32.0)\n",
            "Collecting tensorflow-estimator<2.4.0,>=2.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e9/ed/5853ec0ae380cba4588eab1524e18ece1583b65f7ae0e97321f5ff9dfd60/tensorflow_estimator-2.3.0-py2.py3-none-any.whl (459kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 30.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.1) (1.12.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.1) (0.36.2)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.1) (0.12.0)\n",
            "Collecting numpy<1.19.0,>=1.16.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d6/c6/58e517e8b1fb192725cfa23c01c2e60e4e6699314ee9684a1c5f5c9b27e1/numpy-1.18.5-cp37-cp37m-manylinux1_x86_64.whl (20.1MB)\n",
            "\u001b[K     |████████████████████████████████| 20.1MB 1.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorboard<3,>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.1) (2.4.1)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.1) (1.6.3)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.1) (2.10.0)\n",
            "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.1) (1.1.2)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.1) (0.3.3)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.1) (3.12.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.1) (1.15.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.1) (1.1.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.1) (3.3.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.1) (0.2.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.1) (2.23.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.1) (0.4.4)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.1) (56.0.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.1) (1.0.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.1) (1.28.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.1) (1.8.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.1) (3.3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (1.24.3)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (1.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (4.7.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (4.2.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (3.10.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (3.1.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (0.4.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (3.7.4.3)\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tensorflow-estimator, numpy, tensorflow\n",
            "  Found existing installation: tensorflow-estimator 2.4.0\n",
            "    Uninstalling tensorflow-estimator-2.4.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.4.0\n",
            "  Found existing installation: numpy 1.19.5\n",
            "    Uninstalling numpy-1.19.5:\n",
            "      Successfully uninstalled numpy-1.19.5\n",
            "  Found existing installation: tensorflow 2.4.1\n",
            "    Uninstalling tensorflow-2.4.1:\n",
            "      Successfully uninstalled tensorflow-2.4.1\n",
            "Successfully installed numpy-1.18.5 tensorflow-2.3.1 tensorflow-estimator-2.3.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-quantum\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/53/02/878b2d4e7711f5c7f8dff9ff838e8ed84d218a359154ce06c7c01178a125/tensorflow_quantum-0.4.0-cp37-cp37m-manylinux2010_x86_64.whl (5.9MB)\n",
            "\u001b[K     |████████████████████████████████| 5.9MB 12.6MB/s \n",
            "\u001b[?25hCollecting cirq==0.9.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/05/39c24828744b91f658fd1e5d105a9d168da43698cfaec006179c7646c71c/cirq-0.9.1-py3-none-any.whl (1.6MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6MB 34.5MB/s \n",
            "\u001b[?25hCollecting sympy==1.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4d/a7/25d5d6b3295537ab90bdbcd21e464633fb4a0684dd9a065da404487625bb/sympy-1.5-py2.py3-none-any.whl (5.6MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6MB 23.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: sortedcontainers~=2.0 in /usr/local/lib/python3.7/dist-packages (from cirq==0.9.1->tensorflow-quantum) (2.3.0)\n",
            "Requirement already satisfied: numpy~=1.16 in /usr/local/lib/python3.7/dist-packages (from cirq==0.9.1->tensorflow-quantum) (1.18.5)\n",
            "Requirement already satisfied: requests~=2.18 in /usr/local/lib/python3.7/dist-packages (from cirq==0.9.1->tensorflow-quantum) (2.23.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from cirq==0.9.1->tensorflow-quantum) (1.1.5)\n",
            "Requirement already satisfied: protobuf~=3.12.0 in /usr/local/lib/python3.7/dist-packages (from cirq==0.9.1->tensorflow-quantum) (3.12.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from cirq==0.9.1->tensorflow-quantum) (1.4.1)\n",
            "Requirement already satisfied: networkx~=2.4 in /usr/local/lib/python3.7/dist-packages (from cirq==0.9.1->tensorflow-quantum) (2.5.1)\n",
            "Requirement already satisfied: google-api-core[grpc]<2.0.0dev,>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from cirq==0.9.1->tensorflow-quantum) (1.26.3)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.7/dist-packages (from cirq==0.9.1->tensorflow-quantum) (3.2.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from cirq==0.9.1->tensorflow-quantum) (3.7.4.3)\n",
            "Collecting freezegun~=0.3.15\n",
            "  Downloading https://files.pythonhosted.org/packages/17/5d/1b9d6d3c7995fff473f35861d674e0113a5f0bd5a72fe0199c3f254665c7/freezegun-0.3.15-py2.py3-none-any.whl\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.7/dist-packages (from sympy==1.5->tensorflow-quantum) (1.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests~=2.18->cirq==0.9.1->tensorflow-quantum) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests~=2.18->cirq==0.9.1->tensorflow-quantum) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests~=2.18->cirq==0.9.1->tensorflow-quantum) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests~=2.18->cirq==0.9.1->tensorflow-quantum) (3.0.4)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->cirq==0.9.1->tensorflow-quantum) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->cirq==0.9.1->tensorflow-quantum) (2.8.1)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf~=3.12.0->cirq==0.9.1->tensorflow-quantum) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf~=3.12.0->cirq==0.9.1->tensorflow-quantum) (56.0.0)\n",
            "Requirement already satisfied: decorator<5,>=4.3 in /usr/local/lib/python3.7/dist-packages (from networkx~=2.4->cirq==0.9.1->tensorflow-quantum) (4.4.2)\n",
            "Requirement already satisfied: google-auth<2.0dev,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.14.0->cirq==0.9.1->tensorflow-quantum) (1.28.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.14.0->cirq==0.9.1->tensorflow-quantum) (1.53.0)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.14.0->cirq==0.9.1->tensorflow-quantum) (20.9)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.29.0; extra == \"grpc\" in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.14.0->cirq==0.9.1->tensorflow-quantum) (1.32.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib~=3.0->cirq==0.9.1->tensorflow-quantum) (1.3.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib~=3.0->cirq==0.9.1->tensorflow-quantum) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib~=3.0->cirq==0.9.1->tensorflow-quantum) (0.10.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.21.1->google-api-core[grpc]<2.0.0dev,>=1.14.0->cirq==0.9.1->tensorflow-quantum) (4.2.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.21.1->google-api-core[grpc]<2.0.0dev,>=1.14.0->cirq==0.9.1->tensorflow-quantum) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.21.1->google-api-core[grpc]<2.0.0dev,>=1.14.0->cirq==0.9.1->tensorflow-quantum) (4.7.2)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2.0dev,>=1.21.1->google-api-core[grpc]<2.0.0dev,>=1.14.0->cirq==0.9.1->tensorflow-quantum) (0.4.8)\n",
            "Installing collected packages: sympy, freezegun, cirq, tensorflow-quantum\n",
            "  Found existing installation: sympy 1.7.1\n",
            "    Uninstalling sympy-1.7.1:\n",
            "      Successfully uninstalled sympy-1.7.1\n",
            "Successfully installed cirq-0.9.1 freezegun-0.3.15 sympy-1.5 tensorflow-quantum-0.4.0\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym) (1.18.5)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.3.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.5.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "mavV-PJL5yuW",
        "outputId": "31937948-e531-4592-f65d-c4312f79e4ab"
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "from collections import deque\n",
        "import tensorflow as tf\n",
        "import tensorflow_quantum as tfq\n",
        "import cirq \n",
        "import sympy\n",
        "import time\n",
        "import datetime # https://stackoverflow.com/questions/10607688/how-to-create-a-file-name-with-the-current-date-time-in-python\n",
        "class QDQN_alt(object):\n",
        "    def __init__(self, action_space, state_space, batch, no_qubits=4) -> None:\n",
        "        super().__init__()\n",
        "        self.action_space = action_space\n",
        "        self.state_space = state_space\n",
        "        self.no_qubits = no_qubits\n",
        "        self.qubits = [cirq.GridQubit(0, i) for i in range(no_qubits)]\n",
        "        self.q_network = self.make_func_approx()\n",
        "        self.learning_rate = 0.01\n",
        "        self.opt = tf.keras.optimizers.Adam(lr=self.learning_rate)\n",
        "        self.buff = 10000\n",
        "        self.batch = batch      \n",
        "        self.states = np.zeros((self.buff, self.state_space))\n",
        "        self.actions = np.zeros((self.buff, 1))\n",
        "        self.rewards = np.zeros((self.buff, 1))\n",
        "        self.dones = np.zeros((self.buff, 1))\n",
        "        self.next_states = np.zeros((self.buff, self.state_space))\n",
        "        # Q Learning\n",
        "        self.gamma = 0.95\n",
        "        self.epsilon = 1.0\n",
        "        self.epsilon_decay = 0.9\n",
        "        self.epsilon_min = 0.01\n",
        "        self.counter = 0\n",
        "        self.date = datetime.date.today()\n",
        "        self.model_name = \"QDQN-{date}_qbits{q}_ADAM_lr{lr}_bs_{bs}_g{g}_eps{ep}_epsmin{epmin}_epsd{epd}\".format(\n",
        "            date=self.date,\n",
        "            q=self.no_qubits,g=self.gamma, \n",
        "            lr=self.learning_rate,\n",
        "            bs=self.batch,\n",
        "            ep=self.epsilon,\n",
        "            epmin=self.epsilon_min,\n",
        "            epd=self.epsilon_decay)\n",
        "        self.msbe = None\n",
        "    def make_func_approx(self):\n",
        "        readout_operators = [cirq.Z(self.qubits[i]) for i in range(2,4)]\n",
        "        inputs = tf.keras.Input(shape=(), dtype=tf.dtypes.string)\n",
        "        diff = tfq.differentiators.ParameterShift()\n",
        "        init = tf.keras.initializers.Zeros\n",
        "        pqc = tfq.layers.PQC(self.make_circuit(self.qubits), readout_operators, differentiator=diff, initializer=init)(inputs)\n",
        "        model = tf.keras.Model(inputs=inputs, outputs=pqc)\n",
        "        return model\n",
        "\n",
        "    def convert_data(self, classical_data, flag=True):\n",
        "        ops = cirq.Circuit()\n",
        "        for i, ang in enumerate(classical_data):\n",
        "            ang = 0 if ang < 0 else 1\n",
        "            ops.append(cirq.rx(np.pi * ang).on(self.qubits[i]))\n",
        "            ops.append(cirq.rz(np.pi * ang).on(self.qubits[i]))\n",
        "        if flag:\n",
        "            return tfq.convert_to_tensor([ops])\n",
        "        else:\n",
        "            return ops\n",
        "\n",
        "    def one_qubit_unitary(self, bit, symbols):\n",
        "        return cirq.Circuit(\n",
        "            cirq.X(bit)**symbols[0],\n",
        "            cirq.Y(bit)**symbols[1],\n",
        "            cirq.Z(bit)**symbols[2])\n",
        "\n",
        "    def two_qubit_pool(self, source_qubit, sink_qubit, symbols):\n",
        "        pool_circuit = cirq.Circuit()\n",
        "        sink_basis_selector = self.one_qubit_unitary(sink_qubit, symbols[0:3])\n",
        "        source_basis_selector = self.one_qubit_unitary(source_qubit, symbols[3:6])\n",
        "        pool_circuit.append(sink_basis_selector)\n",
        "        pool_circuit.append(source_basis_selector)\n",
        "        pool_circuit.append(cirq.CNOT(control=source_qubit, target=sink_qubit))\n",
        "        pool_circuit.append(sink_basis_selector**-1)\n",
        "        return pool_circuit\n",
        "\n",
        "    def make_circuit(self, qubits):\n",
        "        m = cirq.Circuit()\n",
        "        no_vars = self.no_qubits*3*3 + 2*6  \n",
        "        no_vars_str = \"q0:\"+str(no_vars)\n",
        "        print(no_vars_str)\n",
        "        symbols = sympy.symbols(no_vars_str) # 4 qubits * 3 weights per bit * 3 layers + 2 * 6 pooling = 36 + 12 = 48\n",
        "        m += self.layer(symbols[:3*self.no_qubits], qubits)\n",
        "        m += self.layer(symbols[3*self.no_qubits:2*3*self.no_qubits], qubits)\n",
        "        m += self.layer(symbols[2*3*self.no_qubits:3*3*self.no_qubits], qubits)\n",
        "        print(m)\n",
        "        m += self.two_qubit_pool(self.qubits[0], self.qubits[2], symbols[3*3*self.no_qubits:3*3*self.no_qubits+6])\n",
        "        m += self.two_qubit_pool(self.qubits[1], self.qubits[3], symbols[3*3*self.no_qubits+6:])\n",
        "        return m\n",
        "    \n",
        "    def layer(self, weights, qubits):\n",
        "        l = cirq.Circuit()\n",
        "        for i in range(len(qubits) - 1):\n",
        "            l.append(cirq.CNOT(qubits[i], qubits[i+1]))\n",
        "        l.append([cirq.Moment([cirq.rx(weights[j]).on(qubits[j]) for j in range(self.no_qubits)])])\n",
        "        l.append([cirq.Moment([cirq.ry(weights[j + self.no_qubits]).on(qubits[j]) for j in range(self.no_qubits)])])\n",
        "        l.append([cirq.Moment([cirq.rz(weights[j + 2*self.no_qubits]).on(qubits[j]) for j in range(self.no_qubits)])])\n",
        "        return l\n",
        "    \n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        i = self.counter % self.buff\n",
        "        self.states[i] = state\n",
        "        self.actions[i] = action\n",
        "        self.rewards[i] = reward\n",
        "        self.next_states[i] = next_state\n",
        "        self.dones[i] = int(done)\n",
        "        self.counter += 1\n",
        "\n",
        "    def get_action(self, obs):\n",
        "        if random.random() < self.epsilon: \n",
        "            return np.random.choice(self.action_space)\n",
        "        else:\n",
        "            return np.argmax(self.q_network.predict(self.convert_data(obs)))\n",
        "    def train(self):\n",
        "        batch_indices = np.random.choice(min(self.counter, self.buff), self.batch)\n",
        "        state_batch = tfq.convert_to_tensor([self.convert_data(i, False) for i in self.states[batch_indices]])\n",
        "        action_batch = tf.convert_to_tensor(self.actions[batch_indices], dtype=tf.int32)\n",
        "        action_batch = [[i, action_batch[i][0]] for i in range(len(action_batch))]\n",
        "        reward_batch = tf.convert_to_tensor(self.rewards[batch_indices], dtype=tf.float32)\n",
        "        dones_batch = tf.convert_to_tensor(self.dones[batch_indices], dtype=tf.float32)\n",
        "        next_state_batch = tfq.convert_to_tensor([self.convert_data(i, False) for i in self.next_states[batch_indices]])\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            next_q = self.q_network(next_state_batch)\n",
        "            next_q = tf.expand_dims(tf.reduce_max(next_q, axis=1), -1)\n",
        "            y = reward_batch + (1 - dones_batch) * self.gamma * next_q\n",
        "            q_guess = self.q_network(state_batch, training=True)\n",
        "            pred = tf.gather_nd(q_guess, action_batch)\n",
        "            pred = tf.reshape(pred, [self.batch, 1])\n",
        "            msbe = tf.math.reduce_mean(tf.math.square(y - pred))\n",
        "            self.msbe = msbe\n",
        "        grads = tape.gradient(msbe, self.q_network.trainable_variables)\n",
        "        self.opt.apply_gradients(zip(grads, self.q_network.trainable_variables))\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "import os.path\n",
        "from os import path\n",
        "def make_path(p, d):\n",
        "  print(\"Checking if {} exists...\".format(p+d))\n",
        "  if path.exists(p+d) == False:\n",
        "    print(\"making... new directory\")\n",
        "    os.mkdir(p+str(d))\n",
        "  print(\"finished!\")\n",
        "  print(p+str(d))\n",
        "  return p+str(d)\n",
        "\n",
        "ITERATIONS = 200\n",
        "batch_size = 32\n",
        "windows = 50\n",
        "learn_delay = 1000\n",
        "qubits = 4\n",
        "batch_sizes = [32]\n",
        "for b in batch_sizes:\n",
        "  losses = []\n",
        "  cur_loss = 1\n",
        "  env = gym.make(\"CartPole-v1\")\n",
        "  env.seed(0)\n",
        "  agent = QDQN_alt(env.action_space.n, env.observation_space.shape[0], b, qubits)\n",
        "  master_path = make_path(\"/content/drive/MyDrive/quantum_research/cart_pole/quantum_models/\", \"d200+\"+agent.model_name)\n",
        "  rewards = []\n",
        "  losses = []\n",
        "  cur_loss = 1\n",
        "  avg_reward = deque(maxlen=ITERATIONS)\n",
        "  best_avg_reward = -math.inf\n",
        "  rs = deque(maxlen=windows)\n",
        "  epi_times = []\n",
        "  start_time = time.process_time()\n",
        "\n",
        "  for i in range(ITERATIONS):\n",
        "      s1 = env.reset()\n",
        "      total_reward = 0\n",
        "      episode_losses=[]\n",
        "      done = False\n",
        "      episode_start = time.process_time()\n",
        "      while not done:\n",
        "          action = agent.get_action(s1)\n",
        "          s2, reward, done, info = env.step(action)\n",
        "          total_reward += reward\n",
        "          agent.remember(s1, action, reward, s2, done)\n",
        "          if agent.counter > learn_delay and done:\n",
        "              agent.train()\n",
        "              episode_losses.append(agent.msbe)\n",
        "          if done:\n",
        "              rewards.append(total_reward)\n",
        "              rs.append(total_reward)\n",
        "          s1 = s2\n",
        "      avg = np.mean(rs)\n",
        "      avg_reward.append(avg)\n",
        "      if avg > best_avg_reward:\n",
        "          best_avg_reward = avg\n",
        "      if len(episode_losses)>0:\n",
        "          EPISODE_LOSSES= np.asarray(episode_losses)\n",
        "          AVERAGE_EPISODE_LOSS = np.mean(EPISODE_LOSSES)\n",
        "          losses.append(AVERAGE_EPISODE_LOSS)\n",
        "          cur_loss = AVERAGE_EPISODE_LOSS\n",
        "      else:\n",
        "          losses.append(cur_loss)\n",
        "      epi_end = time.process_time() -episode_start\n",
        "      epi_times.append(epi_end)\n",
        "      print(\"\\rEpisode {}/{} || Best average reward {}, Current Iteration Reward {}\".format(i, ITERATIONS, best_avg_reward, total_reward))\n",
        "  reward_file = \"{h}/rewards\".format(h = master_path)\n",
        "  average_file = \"{h}/averages\".format(h=master_path)\n",
        "  times_file = \"{h}/times\".format(h=master_path)\n",
        "  loss_file = \"{h}/loss\".format(h=master_path)\n",
        "  np.save(reward_file , np.asarray(rewards))\n",
        "  np.save(average_file , np.asarray(avg_reward))\n",
        "  np.save(times_file , np.asarray(epi_times))\n",
        "  np.save(loss_file , np.asarray(losses))\n",
        "# plt.ylim(0,200)\n",
        "# plt.plot(rewards, color='olive', label='Reward')\n",
        "# plt.plot(avg_reward, color='red', label='Average')\n",
        "# plt.legend()\n",
        "# plt.ylabel('Reward')\n",
        "# plt.xlabel('Generation')\n",
        "# plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "q0:48\n",
            "(0, 0): ───@───────────Rx(q0)───Ry(q4)───Rz(q8)────@───────────Rx(q12)───Ry(q16)───Rz(q20)───@───────────Rx(q24)───Ry(q28)───Rz(q32)───\n",
            "           │                                       │                                         │\n",
            "(0, 1): ───X───@───────Rx(q1)───Ry(q5)───Rz(q9)────X───@───────Rx(q13)───Ry(q17)───Rz(q21)───X───@───────Rx(q25)───Ry(q29)───Rz(q33)───\n",
            "               │                                       │                                         │\n",
            "(0, 2): ───────X───@───Rx(q2)───Ry(q6)───Rz(q10)───────X───@───Rx(q14)───Ry(q18)───Rz(q22)───────X───@───Rx(q26)───Ry(q30)───Rz(q34)───\n",
            "                   │                                       │                                         │\n",
            "(0, 3): ───────────X───Rx(q3)───Ry(q7)───Rz(q11)───────────X───Rx(q15)───Ry(q19)───Rz(q23)───────────X───Rx(q27)───Ry(q31)───Rz(q35)───\n",
            "Checking if /content/drive/MyDrive/quantum_research/cart_pole/quantum_models/d200+QDQN-2021-04-27_qbits4_ADAM_lr0.01_bs_32_g0.95_eps1.0_epsmin0.01_epsd0.9 exists...\n",
            "making... new directory\n",
            "finished!\n",
            "/content/drive/MyDrive/quantum_research/cart_pole/quantum_models/d200+QDQN-2021-04-27_qbits4_ADAM_lr0.01_bs_32_g0.95_eps1.0_epsmin0.01_epsd0.9\n",
            "Episode 0/200 || Best average reward 12.0, Current Iteration Reward 12.0\n",
            "Episode 1/200 || Best average reward 21.5, Current Iteration Reward 31.0\n",
            "Episode 2/200 || Best average reward 28.0, Current Iteration Reward 41.0\n",
            "Episode 3/200 || Best average reward 29.5, Current Iteration Reward 34.0\n",
            "Episode 4/200 || Best average reward 29.5, Current Iteration Reward 17.0\n",
            "Episode 5/200 || Best average reward 29.5, Current Iteration Reward 10.0\n",
            "Episode 6/200 || Best average reward 29.5, Current Iteration Reward 17.0\n",
            "Episode 7/200 || Best average reward 29.5, Current Iteration Reward 17.0\n",
            "Episode 8/200 || Best average reward 29.5, Current Iteration Reward 31.0\n",
            "Episode 9/200 || Best average reward 29.5, Current Iteration Reward 12.0\n",
            "Episode 10/200 || Best average reward 29.5, Current Iteration Reward 22.0\n",
            "Episode 11/200 || Best average reward 29.5, Current Iteration Reward 28.0\n",
            "Episode 12/200 || Best average reward 29.5, Current Iteration Reward 22.0\n",
            "Episode 13/200 || Best average reward 29.5, Current Iteration Reward 10.0\n",
            "Episode 14/200 || Best average reward 29.5, Current Iteration Reward 11.0\n",
            "Episode 15/200 || Best average reward 29.5, Current Iteration Reward 39.0\n",
            "Episode 16/200 || Best average reward 29.5, Current Iteration Reward 14.0\n",
            "Episode 17/200 || Best average reward 29.5, Current Iteration Reward 15.0\n",
            "Episode 18/200 || Best average reward 29.5, Current Iteration Reward 20.0\n",
            "Episode 19/200 || Best average reward 29.5, Current Iteration Reward 25.0\n",
            "Episode 20/200 || Best average reward 29.5, Current Iteration Reward 46.0\n",
            "Episode 21/200 || Best average reward 29.5, Current Iteration Reward 26.0\n",
            "Episode 22/200 || Best average reward 29.5, Current Iteration Reward 59.0\n",
            "Episode 23/200 || Best average reward 29.5, Current Iteration Reward 16.0\n",
            "Episode 24/200 || Best average reward 29.5, Current Iteration Reward 22.0\n",
            "Episode 25/200 || Best average reward 29.5, Current Iteration Reward 12.0\n",
            "Episode 26/200 || Best average reward 29.5, Current Iteration Reward 11.0\n",
            "Episode 27/200 || Best average reward 29.5, Current Iteration Reward 49.0\n",
            "Episode 28/200 || Best average reward 29.5, Current Iteration Reward 18.0\n",
            "Episode 29/200 || Best average reward 29.5, Current Iteration Reward 17.0\n",
            "Episode 30/200 || Best average reward 29.5, Current Iteration Reward 11.0\n",
            "Episode 31/200 || Best average reward 29.5, Current Iteration Reward 17.0\n",
            "Episode 32/200 || Best average reward 29.5, Current Iteration Reward 9.0\n",
            "Episode 33/200 || Best average reward 29.5, Current Iteration Reward 17.0\n",
            "Episode 34/200 || Best average reward 29.5, Current Iteration Reward 11.0\n",
            "Episode 35/200 || Best average reward 29.5, Current Iteration Reward 16.0\n",
            "Episode 36/200 || Best average reward 29.5, Current Iteration Reward 17.0\n",
            "Episode 37/200 || Best average reward 29.5, Current Iteration Reward 25.0\n",
            "Episode 38/200 || Best average reward 29.5, Current Iteration Reward 12.0\n",
            "Episode 39/200 || Best average reward 29.5, Current Iteration Reward 26.0\n",
            "Episode 40/200 || Best average reward 29.5, Current Iteration Reward 19.0\n",
            "Episode 41/200 || Best average reward 29.5, Current Iteration Reward 24.0\n",
            "Episode 42/200 || Best average reward 29.5, Current Iteration Reward 19.0\n",
            "Episode 43/200 || Best average reward 29.5, Current Iteration Reward 19.0\n",
            "Episode 44/200 || Best average reward 29.5, Current Iteration Reward 13.0\n",
            "Episode 45/200 || Best average reward 29.5, Current Iteration Reward 29.0\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/deprecation.py:574: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use fn_output_signature instead\n",
            "Episode 46/200 || Best average reward 29.5, Current Iteration Reward 53.0\n",
            "Episode 47/200 || Best average reward 29.5, Current Iteration Reward 18.0\n",
            "Episode 48/200 || Best average reward 29.5, Current Iteration Reward 13.0\n",
            "Episode 49/200 || Best average reward 29.5, Current Iteration Reward 62.0\n",
            "Episode 50/200 || Best average reward 29.5, Current Iteration Reward 14.0\n",
            "Episode 51/200 || Best average reward 29.5, Current Iteration Reward 25.0\n",
            "Episode 52/200 || Best average reward 29.5, Current Iteration Reward 9.0\n",
            "Episode 53/200 || Best average reward 29.5, Current Iteration Reward 28.0\n",
            "Episode 54/200 || Best average reward 29.5, Current Iteration Reward 18.0\n",
            "Episode 55/200 || Best average reward 29.5, Current Iteration Reward 88.0\n",
            "Episode 56/200 || Best average reward 29.5, Current Iteration Reward 14.0\n",
            "Episode 57/200 || Best average reward 29.5, Current Iteration Reward 29.0\n",
            "Episode 58/200 || Best average reward 29.5, Current Iteration Reward 27.0\n",
            "Episode 59/200 || Best average reward 29.5, Current Iteration Reward 12.0\n",
            "Episode 60/200 || Best average reward 29.5, Current Iteration Reward 9.0\n",
            "Episode 61/200 || Best average reward 29.5, Current Iteration Reward 29.0\n",
            "Episode 62/200 || Best average reward 29.5, Current Iteration Reward 11.0\n",
            "Episode 63/200 || Best average reward 29.5, Current Iteration Reward 9.0\n",
            "Episode 64/200 || Best average reward 29.5, Current Iteration Reward 9.0\n",
            "Episode 65/200 || Best average reward 29.5, Current Iteration Reward 36.0\n",
            "Episode 66/200 || Best average reward 29.5, Current Iteration Reward 19.0\n",
            "Episode 67/200 || Best average reward 29.5, Current Iteration Reward 9.0\n",
            "Episode 68/200 || Best average reward 29.5, Current Iteration Reward 39.0\n",
            "Episode 69/200 || Best average reward 29.5, Current Iteration Reward 24.0\n",
            "Episode 70/200 || Best average reward 29.5, Current Iteration Reward 10.0\n",
            "Episode 71/200 || Best average reward 29.5, Current Iteration Reward 27.0\n",
            "Episode 72/200 || Best average reward 29.5, Current Iteration Reward 40.0\n",
            "Episode 73/200 || Best average reward 29.5, Current Iteration Reward 149.0\n",
            "Episode 74/200 || Best average reward 29.5, Current Iteration Reward 91.0\n",
            "Episode 75/200 || Best average reward 29.5, Current Iteration Reward 19.0\n",
            "Episode 76/200 || Best average reward 29.5, Current Iteration Reward 10.0\n",
            "Episode 77/200 || Best average reward 29.5, Current Iteration Reward 56.0\n",
            "Episode 78/200 || Best average reward 29.5, Current Iteration Reward 9.0\n",
            "Episode 79/200 || Best average reward 29.5, Current Iteration Reward 30.0\n",
            "Episode 80/200 || Best average reward 29.5, Current Iteration Reward 27.0\n",
            "Episode 81/200 || Best average reward 29.5, Current Iteration Reward 9.0\n",
            "Episode 82/200 || Best average reward 29.5, Current Iteration Reward 11.0\n",
            "Episode 83/200 || Best average reward 29.5, Current Iteration Reward 145.0\n",
            "Episode 84/200 || Best average reward 29.5, Current Iteration Reward 17.0\n",
            "Episode 85/200 || Best average reward 29.5, Current Iteration Reward 9.0\n",
            "Episode 86/200 || Best average reward 29.5, Current Iteration Reward 12.0\n",
            "Episode 87/200 || Best average reward 29.5, Current Iteration Reward 11.0\n",
            "Episode 88/200 || Best average reward 29.5, Current Iteration Reward 8.0\n",
            "Episode 89/200 || Best average reward 31.8, Current Iteration Reward 173.0\n",
            "Episode 90/200 || Best average reward 31.8, Current Iteration Reward 9.0\n",
            "Episode 91/200 || Best average reward 31.8, Current Iteration Reward 9.0\n",
            "Episode 92/200 || Best average reward 31.8, Current Iteration Reward 19.0\n",
            "Episode 93/200 || Best average reward 31.8, Current Iteration Reward 9.0\n",
            "Episode 94/200 || Best average reward 31.8, Current Iteration Reward 9.0\n",
            "Episode 95/200 || Best average reward 31.8, Current Iteration Reward 8.0\n",
            "Episode 96/200 || Best average reward 31.8, Current Iteration Reward 8.0\n",
            "Episode 97/200 || Best average reward 31.8, Current Iteration Reward 9.0\n",
            "Episode 98/200 || Best average reward 32.46, Current Iteration Reward 160.0\n",
            "Episode 99/200 || Best average reward 32.46, Current Iteration Reward 13.0\n",
            "Episode 100/200 || Best average reward 32.52, Current Iteration Reward 66.0\n",
            "Episode 101/200 || Best average reward 32.52, Current Iteration Reward 9.0\n",
            "Episode 102/200 || Best average reward 32.52, Current Iteration Reward 9.0\n",
            "Episode 103/200 || Best average reward 32.52, Current Iteration Reward 23.0\n",
            "Episode 104/200 || Best average reward 32.52, Current Iteration Reward 21.0\n",
            "Episode 105/200 || Best average reward 32.52, Current Iteration Reward 21.0\n",
            "Episode 106/200 || Best average reward 32.52, Current Iteration Reward 9.0\n",
            "Episode 107/200 || Best average reward 32.52, Current Iteration Reward 9.0\n",
            "Episode 108/200 || Best average reward 32.52, Current Iteration Reward 9.0\n",
            "Episode 109/200 || Best average reward 32.52, Current Iteration Reward 9.0\n",
            "Episode 110/200 || Best average reward 32.52, Current Iteration Reward 9.0\n",
            "Episode 111/200 || Best average reward 32.52, Current Iteration Reward 10.0\n",
            "Episode 112/200 || Best average reward 32.52, Current Iteration Reward 25.0\n",
            "Episode 113/200 || Best average reward 32.52, Current Iteration Reward 19.0\n",
            "Episode 114/200 || Best average reward 32.52, Current Iteration Reward 13.0\n",
            "Episode 115/200 || Best average reward 33.08, Current Iteration Reward 186.0\n",
            "Episode 116/200 || Best average reward 33.08, Current Iteration Reward 13.0\n",
            "Episode 117/200 || Best average reward 33.08, Current Iteration Reward 15.0\n",
            "Episode 118/200 || Best average reward 33.68, Current Iteration Reward 69.0\n",
            "Episode 119/200 || Best average reward 33.68, Current Iteration Reward 8.0\n",
            "Episode 120/200 || Best average reward 33.68, Current Iteration Reward 8.0\n",
            "Episode 121/200 || Best average reward 33.68, Current Iteration Reward 9.0\n",
            "Episode 122/200 || Best average reward 33.68, Current Iteration Reward 9.0\n",
            "Episode 123/200 || Best average reward 33.68, Current Iteration Reward 8.0\n",
            "Episode 124/200 || Best average reward 33.68, Current Iteration Reward 10.0\n",
            "Episode 125/200 || Best average reward 33.68, Current Iteration Reward 8.0\n",
            "Episode 126/200 || Best average reward 33.68, Current Iteration Reward 37.0\n",
            "Episode 127/200 || Best average reward 33.68, Current Iteration Reward 8.0\n",
            "Episode 128/200 || Best average reward 33.68, Current Iteration Reward 19.0\n",
            "Episode 129/200 || Best average reward 33.68, Current Iteration Reward 30.0\n",
            "Episode 130/200 || Best average reward 33.68, Current Iteration Reward 33.0\n",
            "Episode 131/200 || Best average reward 33.68, Current Iteration Reward 37.0\n",
            "Episode 132/200 || Best average reward 33.68, Current Iteration Reward 25.0\n",
            "Episode 133/200 || Best average reward 33.68, Current Iteration Reward 9.0\n",
            "Episode 134/200 || Best average reward 33.68, Current Iteration Reward 9.0\n",
            "Episode 135/200 || Best average reward 33.68, Current Iteration Reward 196.0\n",
            "Episode 136/200 || Best average reward 33.68, Current Iteration Reward 10.0\n",
            "Episode 137/200 || Best average reward 33.68, Current Iteration Reward 88.0\n",
            "Episode 138/200 || Best average reward 33.68, Current Iteration Reward 25.0\n",
            "Episode 139/200 || Best average reward 33.68, Current Iteration Reward 34.0\n",
            "Episode 140/200 || Best average reward 33.68, Current Iteration Reward 173.0\n",
            "Episode 141/200 || Best average reward 33.68, Current Iteration Reward 9.0\n",
            "Episode 142/200 || Best average reward 33.68, Current Iteration Reward 32.0\n",
            "Episode 143/200 || Best average reward 33.68, Current Iteration Reward 8.0\n",
            "Episode 144/200 || Best average reward 33.68, Current Iteration Reward 9.0\n",
            "Episode 145/200 || Best average reward 34.46, Current Iteration Reward 138.0\n",
            "Episode 146/200 || Best average reward 34.94, Current Iteration Reward 32.0\n",
            "Episode 147/200 || Best average reward 38.06, Current Iteration Reward 165.0\n",
            "Episode 148/200 || Best average reward 38.06, Current Iteration Reward 9.0\n",
            "Episode 149/200 || Best average reward 38.32, Current Iteration Reward 177.0\n",
            "Episode 150/200 || Best average reward 39.98, Current Iteration Reward 149.0\n",
            "Episode 151/200 || Best average reward 39.98, Current Iteration Reward 9.0\n",
            "Episode 152/200 || Best average reward 40.14, Current Iteration Reward 17.0\n",
            "Episode 153/200 || Best average reward 40.14, Current Iteration Reward 9.0\n",
            "Episode 154/200 || Best average reward 40.14, Current Iteration Reward 27.0\n",
            "Episode 155/200 || Best average reward 40.14, Current Iteration Reward 15.0\n",
            "Episode 156/200 || Best average reward 40.32, Current Iteration Reward 32.0\n",
            "Episode 157/200 || Best average reward 41.84, Current Iteration Reward 85.0\n",
            "Episode 158/200 || Best average reward 41.84, Current Iteration Reward 9.0\n",
            "Episode 159/200 || Best average reward 42.98, Current Iteration Reward 66.0\n",
            "Episode 160/200 || Best average reward 43.82, Current Iteration Reward 51.0\n",
            "Episode 161/200 || Best average reward 44.12, Current Iteration Reward 25.0\n",
            "Episode 162/200 || Best average reward 44.24, Current Iteration Reward 31.0\n",
            "Episode 163/200 || Best average reward 44.58, Current Iteration Reward 36.0\n",
            "Episode 164/200 || Best average reward 44.96, Current Iteration Reward 32.0\n",
            "Episode 165/200 || Best average reward 44.96, Current Iteration Reward 29.0\n",
            "Episode 166/200 || Best average reward 44.96, Current Iteration Reward 29.0\n",
            "Episode 167/200 || Best average reward 44.96, Current Iteration Reward 10.0\n",
            "Episode 168/200 || Best average reward 44.96, Current Iteration Reward 9.0\n",
            "Episode 169/200 || Best average reward 44.96, Current Iteration Reward 42.0\n",
            "Episode 170/200 || Best average reward 44.96, Current Iteration Reward 32.0\n",
            "Episode 171/200 || Best average reward 44.96, Current Iteration Reward 134.0\n",
            "Episode 172/200 || Best average reward 44.96, Current Iteration Reward 11.0\n",
            "Episode 173/200 || Best average reward 47.4, Current Iteration Reward 151.0\n",
            "Episode 174/200 || Best average reward 47.86, Current Iteration Reward 33.0\n",
            "Episode 175/200 || Best average reward 47.88, Current Iteration Reward 9.0\n",
            "Episode 176/200 || Best average reward 47.88, Current Iteration Reward 9.0\n",
            "Episode 177/200 || Best average reward 47.94, Current Iteration Reward 39.0\n",
            "Episode 178/200 || Best average reward 47.94, Current Iteration Reward 9.0\n",
            "Episode 179/200 || Best average reward 47.94, Current Iteration Reward 10.0\n",
            "Episode 180/200 || Best average reward 47.94, Current Iteration Reward 10.0\n",
            "Episode 181/200 || Best average reward 47.94, Current Iteration Reward 28.0\n",
            "Episode 182/200 || Best average reward 47.94, Current Iteration Reward 13.0\n",
            "Episode 183/200 || Best average reward 47.94, Current Iteration Reward 71.0\n",
            "Episode 184/200 || Best average reward 47.96, Current Iteration Reward 22.0\n",
            "Episode 185/200 || Best average reward 47.96, Current Iteration Reward 32.0\n",
            "Episode 186/200 || Best average reward 47.96, Current Iteration Reward 31.0\n",
            "Episode 187/200 || Best average reward 47.96, Current Iteration Reward 21.0\n",
            "Episode 188/200 || Best average reward 47.96, Current Iteration Reward 23.0\n",
            "Episode 189/200 || Best average reward 47.96, Current Iteration Reward 24.0\n",
            "Episode 190/200 || Best average reward 47.96, Current Iteration Reward 9.0\n",
            "Episode 191/200 || Best average reward 47.96, Current Iteration Reward 9.0\n",
            "Episode 192/200 || Best average reward 47.96, Current Iteration Reward 95.0\n",
            "Episode 193/200 || Best average reward 47.96, Current Iteration Reward 8.0\n",
            "Episode 194/200 || Best average reward 47.96, Current Iteration Reward 10.0\n",
            "Episode 195/200 || Best average reward 47.96, Current Iteration Reward 9.0\n",
            "Episode 196/200 || Best average reward 47.96, Current Iteration Reward 8.0\n",
            "Episode 197/200 || Best average reward 47.96, Current Iteration Reward 9.0\n",
            "Episode 198/200 || Best average reward 47.96, Current Iteration Reward 11.0\n",
            "Episode 199/200 || Best average reward 47.96, Current Iteration Reward 242.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJagoEM3-cp0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}